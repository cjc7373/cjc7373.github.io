<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Distributed System on Coherence's Blog</title><link>https://blog.coherence.codes/tags/distributed-system/</link><description>Recent content in Distributed System on Coherence's Blog</description><generator>Hugo</generator><language>zh-cn</language><copyright>Coherence. 本站遵循 CC BY-NC-SA 4.0 协议</copyright><lastBuildDate>Sun, 02 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.coherence.codes/tags/distributed-system/index.xml" rel="self" type="application/rss+xml"/><item><title>MIT 6.824 学习笔记(二) Raft</title><link>https://blog.coherence.codes/posts/2023/mit_6.824_2_raft/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate><guid>https://blog.coherence.codes/posts/2023/mit_6.824_2_raft/</guid><description>&lt;p&gt;本文主要是对 Raft 论文的翻译，为了保持准确性，我会尽量使用英文术语。&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;在过去十年，Leslie Lamport 的 Paxos 协议几乎成为了共识的同义词。Paxos 首先定义了一种协议来对单个决定达成共识, 比如一条单个的 log entry, 这被称为 single-decree Paxos。 其支持多个决定的版本 (比如 log) 被称为 muti-Paxos。然而，Paxos 的缺点是难以理解，并且没有提供一个良好的基础来构建可行的实现。&lt;/p&gt;



 &lt;blockquote&gt;
 &lt;p&gt;试图为这个主题增添一点幽默的尝试以惨淡的失败告终。……这个希腊寓言显然使阅读论文的人们分心了，以致于他们无法理解这个算法。我把论文发给了一些人，其中包括 Nancy Lynch, Vassos Hadzilacos 和 Phil Bernstein，他们声称读过了论文。几个月后我发邮件给他们问了如下问题：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;你能否实现一个分布式数据库，它能容忍任何进程的故障（可能是所有进程）而不牺牲一致性，并且在超过半数进程恢复之后继续正常工作？
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;没有人察觉到这个问题和 Paxos 算法之间有任何联系。&lt;/p&gt;
&lt;p&gt;—— Leslie Lamport &lt;a href="https://lamport.azurewebsites.net/pubs/pubs.html?from=https://research.microsoft.com/users/lamport/pubs/pubs.html&amp;amp;type=path#lamport-paxos"&gt;对 The Part-Time Parliament 的评论&lt;/a&gt;&lt;/p&gt;

 &lt;/blockquote&gt;
&lt;p&gt;相较于 Paxos，Raft 的目标是易于理解且符合直觉。为了使 Raft 易于理解，作者采取了解耦 (Raft 将共识问题分解成几个子问题 leader election, log replication, safety, and membership changes) 和缩减状态空间的方式。&lt;/p&gt;
&lt;p&gt;Raft 和已有的共识算法类似（尤其是 Viewstamped Replication），但它有一些新特性。Raft 采取了强 leader 的设计，例如 log entry 只会从 leader 向其他节点分发。这可能是为了性能考虑（比无 leader 要更快，RPC 也更少）Raft 采用基于随机计时器的 leader 选举, 从而用一种简单的方法来解决冲突。另外还有处理成员变更方面的改进。&lt;/p&gt;</description></item><item><title>MIT 6.824 学习笔记(一) GFS</title><link>https://blog.coherence.codes/posts/2023/mit_6.824_1_gfs/</link><pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate><guid>https://blog.coherence.codes/posts/2023/mit_6.824_1_gfs/</guid><description>&lt;p&gt;存储系统通常是一个分布式系统的基石，通常应用可以是无状态的，而所有状态便由存储系统来管理。&lt;/p&gt;
&lt;p&gt;Google 文件系统（The Google File System, GFS）在 2003 年于一篇同名论文中被提出，发表在系统领域顶会 SOSP 上，是 Google 大数据三驾马车之一&lt;sup id="fnref:1"&gt;&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref"&gt;1&lt;/a&gt;&lt;/sup&gt;。GFS 是一个成功的系统，在 00 年代早期，人们对于分布式文件系统已经有很好的理解了，但是尚未有一个可以扩展到上千个节点的系统被实现出来。很多 GFS 的设计被 HDFS 等后来的分布式文件系统上。&lt;/p&gt;
&lt;p&gt;GFS 基于以下几个方面的观察：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点故障是常见的，因为其运行在大量普通机器（commodity components）上&lt;/li&gt;
&lt;li&gt;存储的文件以大文件（数百 MB 到数 GB）为主，小文件应当被支持，但不应对其优化&lt;/li&gt;
&lt;li&gt;对文件的大多数写入操作是追加 （append）而不是覆盖/随机写，大多数读取操作是顺序读 （1 MB 或更多）
&lt;ul&gt;
&lt;li&gt;所以系统必须有良好的并发追加操作的语义&lt;/li&gt;
&lt;li&gt;工作负载通常是批处理任务，所以高吞吐量比低延时重要&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;同时设计应用和文件系统 API 有利于整个系统的灵活性&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="架构"&gt;架构&lt;/h2&gt;
&lt;p&gt;&lt;img src="./image-20230901145431686.png" alt="image-20230901145431686"&gt;&lt;/p&gt;
&lt;p&gt;GFS 采用 Master/Slave 架构，集群中存在一个 master 和多个 chunkservers，并且被多个 clients 访问。（由于单 master 的存在，GFS 存在单点故障的可能性，虽然 master 同样有备份，但恢复可能需要人工干预）一个文件将被分割成多个 chunk。chunk 的特性如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;固定大小，Google 的选择是 64 MB，这么大的 chunk 将有如下好处：
&lt;ul&gt;
&lt;li&gt;减少 client 和 master 的通信，因为在获取到 chunk 的信息后，client 只需要和 chunkserver 交互进行读写&lt;/li&gt;
&lt;li&gt;减少 metadata 的尺寸，以便所有 metadata 可以保存在内存中&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;作为一个普通的 Linux 文件存储在 chunkserver 上&lt;/li&gt;
&lt;li&gt;每个 chunk 由一个不可变的、唯一的 64 位 chunk handle 标记，其由 master 在 chunk 创建时指定
&lt;ul&gt;
&lt;li&gt;chunkserver 根据 chunk handle 和偏移（byte range）来读写 chunk&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;为了可靠性，每个 chunk 会被复制到多个 chunkservers 上 （通常为三个）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;master 管理了所有的元数据，用 Go 代码大概表示为：&lt;/p&gt;</description></item></channel></rss>