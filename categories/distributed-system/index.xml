<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Distributed System on Coherence's Blog</title><link>https://blog.coherence.codes/categories/distributed-system/</link><description>Recent content in Distributed System on Coherence's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Coherence. 本站遵循 CC BY-NC-SA 4.0 协议</copyright><lastBuildDate>Tue, 05 Sep 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.coherence.codes/categories/distributed-system/index.xml" rel="self" type="application/rss+xml"/><item><title>MIT 6.824 学习笔记(一) GFS</title><link>https://blog.coherence.codes/posts/2023/mit_6.824_1_gfs/</link><pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate><guid>https://blog.coherence.codes/posts/2023/mit_6.824_1_gfs/</guid><description>Google 文件系统（The Google File System, GFS）在 2003 年于一篇同名论文中被提出，发表在系统领域顶会 SOSP 上，是 Google 大数据三驾马车之一1。GFS 是一个成功的系统，在 00 年代早期，人们对于分布式文件系统已经有很好的理解了，但是尚未有一个可以扩展到上千个节点的系统被实现出来。很多 GFS 的设计被 HDFS 等后来的分布式文件系统上。
GFS 基于以下几个方面的观察：
节点故障是常见的，因为其运行在大量普通机器（commodity components）上 存储的文件以大文件（数百 MB 到数 GB）为主，小文件应当被支持，但不应对其优化 对文件的大多数写入操作是追加 （append）而不是覆盖/随机写，大多数读取操作是顺序读 （1 MB 或更多） 所以系统必须有良好的并发追加操作的语义 工作负载通常是批处理任务，所以高吞吐量比低延时重要 同时设计应用和文件系统 API 有利于整个系统的灵活性 架构 GFS 采用 Master/Slave 架构，集群中存在一个 master 和多个 chunkservers，并且被多个 clients 访问。（由于单 master 的存在，GFS 存在单点故障的可能性，虽然 master 同样有备份，但恢复可能需要人工干预）一个文件将被分割成多个 chunk。chunk 的特性如下：
固定大小，Google 的选择是 64 MB，这么大的 chunk 将有如下好处： 减少 client 和 master 的通信，因为在获取到 chunk 的信息后，client 只需要和 chunkserver 交互进行读写 减少 metadata 的尺寸，以便所有 metadata 可以保存在内存中 作为一个普通的 Linux 文件存储在 chunkserver 上 每个 chunk 由一个不可变的、唯一的 64 位 chunk handle 标记，其由 master 在 chunk 创建时指定 chunkserver 根据 chunk handle 和偏移（byte range）来读写 chunk 为了可靠性，每个 chunk 会被复制到多个 chunkservers 上 （通常为三个） master 管理了所有的元数据，用 Go 代码大概表示为：</description></item></channel></rss>